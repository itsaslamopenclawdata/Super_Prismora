# Logstash pipeline configuration for PhotoIdentifier platform
# Collects, parses, and forwards logs to Elasticsearch

input {
  # Collect logs from application containers via Gelf
  gelf {
    port => 5000
    type => "gelf"
    codec => "json"
  }

  # Collect logs via UDP
  udp {
    port => 5000
    type => "udp"
    codec => "json"
  }

  # Collect logs via TCP
  tcp {
    port => 5000
    type => "tcp"
    codec => "json_lines"
  }
}

filter {
  # Parse JSON if needed
  if [type] == "tcp" and [message] =~ /^{.*}$/ {
    json {
      source => "message"
    }
  }

  # Add application context
  mutate {
    add_field => {
      "[@metadata][application]" => "photoidentifier"
    }
  }

  # Extract service name from log source
  if [container_name] {
    grok {
      match => { "container_name" => "/(?<service>[\w-]+)" }
    }
  }

  # Parse log levels
  if [message] =~ /(ERROR|error|Error)/ {
    mutate {
      add_field => { "log_level" => "error" }
    }
  } else if [message] =~ /(WARN|warn|Warning)/ {
    mutate {
      add_field => { "log_level" => "warning" }
    }
  } else if [message] =~ /(INFO|info)/ {
    mutate {
      add_field => { "log_level" => "info" }
    }
  } else if [message] =~ /(DEBUG|debug)/ {
    mutate {
      add_field => { "log_level" => "debug" }
    }
  }

  # Extract timestamp from logs
  date {
    match => [ "timestamp", "ISO8601", "UNIX" ]
  }

  # Remove sensitive data
  mutate {
    remove_field => [ "password", "token", "api_key", "secret" ]
  }
}

output {
  # Send all logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "photoidentifier-logs-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }

  # Optional: Log to console for debugging
  stdout {
    codec => rubydebug
  }
}
