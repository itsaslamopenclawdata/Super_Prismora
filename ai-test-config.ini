# AI Model Testing Configuration

[pytest]
# Additional configuration for AI/ML model testing

# AI-specific test markers
markers =
    ai: marks tests as AI/ML model tests
    model: marks tests as model evaluation tests
    accuracy: marks tests that evaluate model accuracy
    performance: marks tests that evaluate model performance
    robustness: marks tests that evaluate model robustness
    slow: marks slow tests that require more time/resources

# AI test settings
timeout = 300  # 5 minutes for AI tests
timeout_method = thread  # Use thread-based timeout

# Coverage for AI models
[coverage:run]
omit =
    */tests/*
    */migrations/*
    */venv/*
    */__pycache__/*
    */node_modules/*
    */models/*  # Exclude large model files from coverage

[coverage:report]
precision = 2
show_missing = True

# AI Model Evaluation Settings
[ai:evaluation]
# Minimum accuracy thresholds
min_accuracy = 0.90  # 90% overall accuracy
min_per_class_accuracy = 0.85  # 85% per-class accuracy
min_confidence_calibration = 0.80  # 80% calibration score

# Performance thresholds
max_inference_time_ms = 2000  # 2 seconds per image
max_avg_inference_time_ms = 1500  # 1.5 seconds average
max_memory_mb = 500  # 500 MB memory usage

# Robustness thresholds
min_success_rate = 0.80  # 80% success rate on edge cases

# Test dataset configuration
[ai:dataset]
test_data_path = data/test
validation_data_path = data/validation
ground_truth_labels = data/labels.json

# Model configuration
[ai:model]
model_path = models/photo-identifier-v1
batch_size = 32
input_size = [224, 224, 3]
supported_formats = jpg, jpeg, png, webp

# Logging configuration
[ai:logging]
log_level = INFO
log_predictions = true
log_performance_metrics = true
save_evaluation_reports = true
report_path = reports/model-evaluation
